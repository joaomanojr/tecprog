{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frozen_lake_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjL71DdCmeNRXLsDW6wUd6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaomanojr/tecprog/blob/main/frozen_lake_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5uBDsEmvXhY"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch as T\n",
        "\n",
        "class LinearDeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, n_actions, input):\n",
        "        super(LinearDeepQNetwork, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input, 128)\n",
        "        self.fc2 = nn.Linear(128, n_actions)\n",
        "\n",
        "        # Author: self.parameters() from inherited class Module\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        # Author: pytorch have different tensors for cuda/cpu devices\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        layer1 = F.relu(self.fc1(state))\n",
        "        # Author: MSELoss will take care of activation for us...\n",
        "        actions = self.fc2(layer1)\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, lr, n_actions, gamma=0.95,\n",
        "                 epsilon=1.0, eps_dec=1e-5, eps_min=0.01):\n",
        "        \"\"\" Agent init takes:\n",
        "        --\n",
        "        lr - alpha learning rate factor\n",
        "        input_dims - from our environment dimensions\n",
        "        n_actions - actions space dimension\n",
        "        gamma - discount factor on MDP rewards\n",
        "        epsilon - Epsilon Greedy initial value (exploration threshold)\n",
        "        eps_dec - Epsilon Greedy decrease factor\n",
        "        eps_min - Epsilon Greedy minimum, final value (must be > 0)\n",
        "        \n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        # Joao: hardcoded as 1\n",
        "        self.input_dims = 1\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_dec = eps_dec\n",
        "        self.eps_min = eps_min\n",
        "        self.action_space = [i for i in range(self.n_actions)]\n",
        "\n",
        "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        ''' Choose Epsilon Greedy action for a given state '''\n",
        "        if np.random.random() > self.epsilon:\n",
        "            state = T.tensor(observation, dtype=T.float).to(self.Q.device)\n",
        "            # https://stackoverflow.com/questions/64192810/runtime-error-both-arguments-to-matmul-need-to-be-at-least-1d-but-they-are-0d\n",
        "            actions = self.Q.forward(state.unsqueeze(dim=0))\n",
        "            action = T.argmax(actions).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def decrement_epsilon(self):\n",
        "        ''' Epsilon decrease function (linear) '''\n",
        "        # Look: my beloved C ternary in python terms!\n",
        "        self.epsilon = self.epsilon - self.eps_dec \\\n",
        "                        if self.epsilon > self.eps_min else self.eps_min\n",
        "    \n",
        "    def learn(self, state, action, reward, state_):\n",
        "        \"\"\" Off Policy (always Greedy) Learn function \n",
        "        --\n",
        "        Here defined as plain Bellman equation, state_ is state'\n",
        "        \"\"\"\n",
        "        self.Q.optimizer.zero_grad()\n",
        "\n",
        "        states = T.tensor(state, dtype=T.float).to(self.Q.device)\n",
        "        actions = T.tensor(action).to(self.Q.device)\n",
        "        rewards = T.tensor(reward).to(self.Q.device)\n",
        "        states_ = T.tensor(state_, dtype=T.float).to(self.Q.device)\n",
        "\n",
        "        q_pred = self.Q.forward(states.unsqueeze(dim=0))[actions]\n",
        "        q_next = self.Q.forward(states_.unsqueeze(dim=0)).max()\n",
        "\n",
        "        q_target = reward + self.gamma*q_next\n",
        "\n",
        "        # evaluate loss (cost) as the difference at better known and actual\n",
        "        # action.\n",
        "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
        "        # Author: backpropagate cost and add a step on our optimizer.\n",
        "        # These two calls are critical for learn loop.\n",
        "        loss.backward()\n",
        "\n",
        "        self.Q.optimizer.step()\n",
        "        self.decrement_epsilon()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rd6W1qTCvmew",
        "outputId": "37d15a8b-0ea7-4c19-cfa9-e2df516a270c"
      },
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "n_games = 10000\n",
        "scores = []\n",
        "win_pct_list = []\n",
        "\n",
        "agent = Agent(lr=0.0001, n_actions=4)\n",
        "\n",
        "for i in range(n_games):\n",
        "    score = 0\n",
        "    done = False\n",
        "    # This looks like a context pointer to created environment, here we are\n",
        "    # just initializing it - it returns our current state also;\n",
        "    obs = env.reset()\n",
        "\n",
        "    # Interact with environment until done (end or fall in a hole)\n",
        "    while not done:\n",
        "        action = agent.choose_action(obs)\n",
        "        obs_, reward, done, info = env.step(action)\n",
        "        #env.render()\n",
        "        score += reward\n",
        "        agent.learn(obs, action, reward, obs_)\n",
        "        obs = obs_\n",
        "    scores.append(score)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        win_pct = np.mean(scores[-100:])\n",
        "        win_pct_list.append(win_pct)\n",
        "        print('episode', i, 'win pct %.2f' % win_pct,\n",
        "              'epsilon %.2f' % agent.epsilon)\n",
        "\n",
        "plt.plot(win_pct_list)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 0 win pct 0.00 epsilon 1.00\n",
            "episode 100 win pct 0.01 epsilon 0.99\n",
            "episode 200 win pct 0.02 epsilon 0.98\n",
            "episode 300 win pct 0.02 epsilon 0.98\n",
            "episode 400 win pct 0.00 epsilon 0.97\n",
            "episode 500 win pct 0.00 epsilon 0.96\n",
            "episode 600 win pct 0.01 epsilon 0.95\n",
            "episode 700 win pct 0.03 epsilon 0.95\n",
            "episode 800 win pct 0.03 epsilon 0.94\n",
            "episode 900 win pct 0.01 epsilon 0.93\n",
            "episode 1000 win pct 0.01 epsilon 0.92\n",
            "episode 1100 win pct 0.01 epsilon 0.92\n",
            "episode 1200 win pct 0.02 epsilon 0.91\n",
            "episode 1300 win pct 0.02 epsilon 0.90\n",
            "episode 1400 win pct 0.01 epsilon 0.89\n",
            "episode 1500 win pct 0.02 epsilon 0.89\n",
            "episode 1600 win pct 0.03 epsilon 0.88\n",
            "episode 1700 win pct 0.02 epsilon 0.87\n",
            "episode 1800 win pct 0.03 epsilon 0.86\n",
            "episode 1900 win pct 0.01 epsilon 0.86\n",
            "episode 2000 win pct 0.02 epsilon 0.85\n",
            "episode 2100 win pct 0.04 epsilon 0.84\n",
            "episode 2200 win pct 0.03 epsilon 0.83\n",
            "episode 2300 win pct 0.00 epsilon 0.82\n",
            "episode 2400 win pct 0.02 epsilon 0.82\n",
            "episode 2500 win pct 0.01 epsilon 0.81\n",
            "episode 2600 win pct 0.01 epsilon 0.80\n",
            "episode 2700 win pct 0.00 epsilon 0.79\n",
            "episode 2800 win pct 0.01 epsilon 0.78\n",
            "episode 2900 win pct 0.00 epsilon 0.78\n",
            "episode 3000 win pct 0.00 epsilon 0.77\n",
            "episode 3100 win pct 0.00 epsilon 0.76\n",
            "episode 3200 win pct 0.02 epsilon 0.75\n",
            "episode 3300 win pct 0.01 epsilon 0.75\n",
            "episode 3400 win pct 0.02 epsilon 0.74\n",
            "episode 3500 win pct 0.02 epsilon 0.73\n",
            "episode 3600 win pct 0.01 epsilon 0.72\n",
            "episode 3700 win pct 0.02 epsilon 0.72\n",
            "episode 3800 win pct 0.02 epsilon 0.71\n",
            "episode 3900 win pct 0.03 epsilon 0.70\n",
            "episode 4000 win pct 0.02 epsilon 0.69\n",
            "episode 4100 win pct 0.02 epsilon 0.68\n",
            "episode 4200 win pct 0.01 epsilon 0.67\n",
            "episode 4300 win pct 0.01 epsilon 0.67\n",
            "episode 4400 win pct 0.00 epsilon 0.66\n",
            "episode 4500 win pct 0.00 epsilon 0.65\n",
            "episode 4600 win pct 0.02 epsilon 0.64\n",
            "episode 4700 win pct 0.02 epsilon 0.64\n",
            "episode 4800 win pct 0.00 epsilon 0.63\n",
            "episode 4900 win pct 0.03 epsilon 0.62\n",
            "episode 5000 win pct 0.02 epsilon 0.61\n",
            "episode 5100 win pct 0.02 epsilon 0.60\n",
            "episode 5200 win pct 0.02 epsilon 0.60\n",
            "episode 5300 win pct 0.00 epsilon 0.59\n",
            "episode 5400 win pct 0.01 epsilon 0.58\n",
            "episode 5500 win pct 0.00 epsilon 0.57\n",
            "episode 5600 win pct 0.01 epsilon 0.56\n",
            "episode 5700 win pct 0.03 epsilon 0.55\n",
            "episode 5800 win pct 0.00 epsilon 0.55\n",
            "episode 5900 win pct 0.02 epsilon 0.54\n",
            "episode 6000 win pct 0.02 epsilon 0.53\n",
            "episode 6100 win pct 0.01 epsilon 0.52\n",
            "episode 6200 win pct 0.04 epsilon 0.51\n",
            "episode 6300 win pct 0.02 epsilon 0.50\n",
            "episode 6400 win pct 0.01 epsilon 0.50\n",
            "episode 6500 win pct 0.00 epsilon 0.49\n",
            "episode 6600 win pct 0.03 epsilon 0.48\n",
            "episode 6700 win pct 0.01 epsilon 0.47\n",
            "episode 6800 win pct 0.02 epsilon 0.47\n",
            "episode 6900 win pct 0.05 epsilon 0.46\n",
            "episode 7000 win pct 0.00 epsilon 0.45\n",
            "episode 7100 win pct 0.02 epsilon 0.44\n",
            "episode 7200 win pct 0.01 epsilon 0.43\n",
            "episode 7300 win pct 0.03 epsilon 0.42\n",
            "episode 7400 win pct 0.01 epsilon 0.42\n",
            "episode 7500 win pct 0.01 epsilon 0.41\n",
            "episode 7600 win pct 0.03 epsilon 0.40\n",
            "episode 7700 win pct 0.01 epsilon 0.39\n",
            "episode 7800 win pct 0.00 epsilon 0.38\n",
            "episode 7900 win pct 0.00 epsilon 0.37\n",
            "episode 8000 win pct 0.00 epsilon 0.37\n",
            "episode 8100 win pct 0.02 epsilon 0.36\n",
            "episode 8200 win pct 0.00 epsilon 0.35\n",
            "episode 8300 win pct 0.00 epsilon 0.34\n",
            "episode 8400 win pct 0.00 epsilon 0.34\n",
            "episode 8500 win pct 0.02 epsilon 0.33\n",
            "episode 8600 win pct 0.01 epsilon 0.32\n",
            "episode 8700 win pct 0.01 epsilon 0.31\n",
            "episode 8800 win pct 0.03 epsilon 0.30\n",
            "episode 8900 win pct 0.00 epsilon 0.30\n",
            "episode 9000 win pct 0.01 epsilon 0.29\n",
            "episode 9100 win pct 0.00 epsilon 0.28\n",
            "episode 9200 win pct 0.02 epsilon 0.27\n",
            "episode 9300 win pct 0.01 epsilon 0.26\n",
            "episode 9400 win pct 0.00 epsilon 0.26\n",
            "episode 9500 win pct 0.01 epsilon 0.25\n",
            "episode 9600 win pct 0.00 epsilon 0.24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c33d123e406d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}